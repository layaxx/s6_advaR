---
title: "Random Forest und Gradient Boosting in R"
author: "Jonathan Loew, Yannick Lang"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    math: katex
    number_sections: yes
    toc: yes
    toc_depth: 2
    fig_caption: yes
    fig_width: 7
    fig_height: 7
    keep_md: yes
bibliography: [references.bib, packages.bib]  
---

```{r setupMain, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# R Version: 4.0.5
```

# Einleitung
In der heutigen Welt ist es oft nötig eine automatische Klassifikation oder 
Schätzung von Daten durchzuführen. 
Zwei mögliche Methoden dafür sind Random Forest sowie Gradient Boosting.
Im Rahmen dieses Papers werden diese Methoden im Allgemeinen und spezielle 
Implementierungen im Besonderen vorgestellt und verglichen. 
Der Fokus wird hierbei auf Vor- und Nachteilen der beiden Verfahren liegen.

# Einordnung
Random Forest und Gradient Boosting sind grundlegend recht ähnliche Methoden.
Beide Verfahren sind supervised machine learning Verfahren, das heißt die Modelle werden 
anhand bekannter Ergebnisse trainiert.
Auch das Einsatzgebiet, vorwiegend Klassifizierungs- oder Regressionsaufgaben, haben
beide gemeinsam.

Außerdem basieren beide auf dem Prinzip der collective Intelligence, bei dem 
einfachere Methoden kombiniert werden, um bessere Ergenisse zu erzielen.
Bei diesen einfacheren Methoden handelt es sich hier um Entscheidungsbäume.
Ein Entscheidungsbaum wird beginnend beim Wurzelknoten durchlaufen, wobei an jedem Knoten 
eine datenbasierte Entscheidung getroffen wird, die bestimmt, welcher Weg von 
hier aus genommen wird.
Sobald ein Blatt des Baums erreicht wird, endet der Durchlauf. 
Um einen Entscheidungsbaum zu trainieren, gibt es verschiedene Methoden, 
diese werden hier allerdings nicht weiter beleuchtet. 
Die Leistung eines einzelnen Entscheidungsbaums kann leicht durch verschiedenen Fehlern wie 
Überanpassung, Verzerrungs- oder Varianzfehler verschlechtert werden. 
Um diese abzuschwächen, kombinieren Random Forest und 
Gradient Boosting mehrere Entscheidungbäume. 

Wie in den folgenden Kapiteln demonstriert werden wird, besteht 
der größte Unterschied der Methoden im Training der Entscheidungsbäume. 

## Übersicht über Random Forest
Ein Random Forest besteht grundlegend aus beliebig vielen unabhängigen Entscheidungsbäumen. 
Jeder dieser Bäume wird aber nur auf einem Teil des gesamten Datensatzes trainiert. 
Hierfür werden für jeden Baum zufällig Attribute und Datenreihen ausgewählt. 
Dadurch werden viele verschiedene Entscheidungsbäume erstellt, welche oftmals 
verschiedene Ergebnisse liefern. 
Deswegen müssen die Ergebnisse der einzelnen Bäume zuletzt noch aggregiert werden. 
Kein einzelner der trainierten Entscheidungsbäume ist besonders gut darin die Daten 
zu klassifizieren oder zu schätzen, zusammengenommen steigt die Leistung
der Bäume jedoch deutlich, insbesondere weil Ausreißer durch die Aggregation weniger
Einfluss haben.

Einfluss auf die Qualität der Ergebnisse (aber natürlich auch die Performanz)
haben hier vor allem zwei Hyperparameter:

- die Anzahl der geschätzten Bäume
- die Anzahl der Features pro Knoten

## Übersicht über Gradient Boosting
Gradient Boosting nutzt die Entscheidungsbäume auf einer anderen Art. 
Wo beim Random Forest alle Bäume unabhängig voneinander waren, 
bauen die Entscheidungsbäume hier aufeinander auf.

Es wird zuerst ein einzelner Entscheidungsbaum auf dem gesamten Datensatz trainiert. 
Wenn man das Datenset nun mit diesem Baum schätzt, kann man die Abweichung als die Differenz zwischen den
tatsächlichen Werten und durch den Baum geschätzten Werten ausrechnen.
Auf diese Abweichung wird ein weiterer Entscheidungsbaum trainiert, 
hierfür wird erneut der komplette Datensatz genommen, 
nur dass das Zielergebnis nun die eben berechnete Abweichung ist. 
Wenn jetzt die Ergebnisse beider Bäume addiert werden, bekommt man nun ein genaueres Ergebnis, 
verglichen mit einem einzigen Entscheidungsbaum. Dieser Prozess kann nun beliebig oft wiederholt werden.

Auch hier gibt es prominente Hyperparameter, die Einfluss auf Qualität und Performanz haben:

- die Anzahl der geschätzten Bäume
- die Tiefe der Bäume
- die Lernrate: soll ein Overfitting des Trainingdatensatzes vermeiden


```{r child="Packages.Rmd"}

```


# Vor- und Nachteile
Mit den richtigen Parametern performed Gradient boosting in der Regel besser [@Ravanshad].
Es gibt allerdings auch potentielle Nachteile: Weil die einzelnen Entscheidungsbäume beim
Gradient Boosting anders als bei Random Forest nicht unabhängig voneinander sind, können 
sie nicht parallel trainiert werden, was Gradient Boosting tendenziell langsamer macht.
Auch ist die korrekte Wahl der Hyperparameter wichtig, da Gradient Boosting mit den falschen
Parametern leicht overfitted.

Wie die beiden Methoden mit fehlenden Werten (NAs) umgehen, hängt sehr von der konkreten
Implementierung ab. Beispielsweise wirft rForestry´s multilayerForestry Funktion einen Fehler,
wenn der Datensatz NAs enthält, manche Funktionen füllen fehlende Daten aber auch mit Mittelwerten auf.


```{r writeReferences, include=FALSE}

knitr::write_bib(.packages(), "packages.bib")
```


# References
