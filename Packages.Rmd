---
title: 'Random Forest vs. Gradient Boosting'
author: "Jonathan Loew, Yannick Lang"
date: 
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    math: katex
    number_sections: yes
    toc: yes
    toc_depth: 2
    fig_caption: yes
    fig_width: 7
    fig_height: 7
    keep_md: yes
subtitle: 'verschiedene Random Forest / Gradient Boosting packages in R'
---

```{r setupPackages, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require("pacman")) {
  # install pacman if not already installed
  options(repos = list(CRAN = "http://cran.rstudio.com/"))
  install.packages("pacman")
}
```

# Implementierung in R

## Beispieldatensatz
Unsere Beispieldaten bestehen aus dem "Housing Values in Suburbs of Boston" Datensatz, 
als abhängige Werte wollen wir `medv`, den Median Wert der Häuser, schätzen.
```{r dataset}
set.seed(42)
library(pacman)

data <- MASS::Boston # Beispieldatensatz
column <- 14 # Spalte mit den abhängigen Werten
formula <- formula(paste(names(data)[column], "~ ."))
formula
```

## Benchmark
Die Benchmark-Funktion erwartet als Eingabe eine Funktion, die das Model generiert und eine optionale Zahl, die der Anzahl der Wiederholungen enspricht.

Das Model wird `times` Male mit je unterschiedlichen Daten geschätzt und anhand der Testdaten auf Güte überprüft. Ausgegeben werden durchschnittliche Standardabweichung, Differenz zwischen echten Werten des Testdatensatzes und den geschätzten Werten des Models und die durchschnittliche Laufzeit der Modelgenerierung.
```{r benchmark-utils}
# Hilfsfuntion, die den Datensatz in Trainings- und Testdatensatz unterteilt
split_into_training_and_test <- function(dataset) {
  # 20% der Daten sollen Testdaten sein
  test_ids <- sample(nrow(data), nrow(data) * 0.2)

  list(test = data[test_ids, ], training = data[-test_ids, ])
}

get_x_values <- function(dataset) {
  dataset[, -column]
}
get_y_values <- function(dataset) {
  dataset[, column]
}
```


```{r benchmark-single}
run_single_benchmark <- function(create_model) {
  datasets <- split_into_training_and_test(data)

  # Model trainieren und Zeit messen
  start_time <- Sys.time()
  model <- create_model(datasets$training)
  end_time <- Sys.time()

  # geschätzte Werte berechnen
  predicted_values <- predict(model, get_x_values(datasets$test))
  actual_values <- get_y_values(datasets$test)

  # geschätzte Werte mit echten Werten vergleichen
  list(
    sd = sd(actual_values - predicted_values),
    mean = mean(abs(actual_values - predicted_values)),
    duration = end_time - start_time
  )
}
```

```{r benchmark-multi}
benchmark <- function(create_model, times = 1) {
  sums <- list(sd = 0, mean = 0, duration = 0)
  for (idx in 1:times) {
    # benchmark `times` Male ausführen
    result <- run_single_benchmark(create_model)

    # Ergebnisse aufaddieren
    sums$sd <- sums$sd + result$sd
    sums$mean <- sums$mean + result$mean
    sums$duration <- sums$duration + result$duration
  }

  # Durchschnitt der Ergebnisse bilden
  averages <- data.frame(
    mean = sums$mean / times,
    sd = sums$sd / times,
    duration = sums$duration / times
  )

  display_names <- c("Mean", "Standard Deviation", "Duration")
  knitr::kable(averages, col.names = display_names)
}
```


## packages
### Rforestry
Rforestry unterstützt sowohl Random Forests als auch Gradient Boosting.

```{r rforestry-setup}
p_load(Rforestry)
```

Ein Random Forest wird mithilfe der `forestry` Funktion erstellt.

Tuning Parameter [@R-Rforestry]:

- ntree: Anzahl der Bäume, default 500
- maxDepth: maximale Tiefe eines Baums, default 99

```{r rforestry-benchmark}
benchmark(function(data) {
  forestry(x = get_x_values(data), y = get_y_values(data))
}, 10)
```

Gradient Boosting kann mithilfe der `multiLayerForestry` Funktion umgesetzt werden.

Tuning Parameter [@R-Rforestry]:

- ntree: Anzahl der Bäume, default 500
- maxDepth: maximale Tiefe eines Baums, default 99
- eta: Lernrate, default 0.3
```{r rforestry-benchmark-2}
benchmark(function(data) {
  multilayerForestry(x = get_x_values(data), y = get_y_values(data))
}, 10)
```

### gbm
Die `gbm` Bibliothek ist auf Gradient Boosting spezialisiert.

Tuning Parameter [@R-gbm]:

- n.trees: Anzahl der Bäume, default 100
- interaction.depth: maximale Tiefe eines Baums, default 1
- shrinkage: Lernrate, default 0.1
- n.minobsinnode: Mindestanzahl an Beobachtungen in Knoten, default 10

```{r gbm-setup}
p_load(gbm)
```

Benchmark:
```{r gbm-benchmark, message=FALSE}
benchmark(function(data) {
  gbm(formula,
    data = data,
    distribution = "gaussian"
  )
}, 10)
```
 
### caret
```{r caret-setup-dependencies, include=FALSE}
p_install("ggplot2", force = FALSE)
```

```{r caret-setup}
p_load(caret)
```
Caret ist eine sehr vielseitige Bibliothek, die verschiedenste Modelle schätzen kann. Je nach gewünschtem
Modell müssen gegebenenfalls zusätzliche Bibliotheken installiert werden.

Die Hyperparameter hängen von der Funktion ab, die zum Schätzen verwendet werden soll.
Diese werden bei caret allerdings automatisch getuned [@R-caret]. 
Kontrolle darüber geben die Parameter trControl und tuneLength.
Für genauere Kontrolle über die erlaubten Werte kann auch tuneGrid benutzt werden, hierfür werden allerdings die Namen der Parameter gebraucht.

Diese können wie im Folgenden gezeigt ausgegeben werden:

Parameter für Random Forest:
```{r modelInforf}
getModelInfo("rf")$rf$parameters
```

Benchmark für Random Forest:
```{r caret-benchmark-rf}
p_load(randomForest)
benchmark(function(data) {
  caret::train(formula,
    data = data,
    method = "rf",
    verbose = 0
  )
}, 3)
```
*Anmerkung:* Da das Trainieren eines Random Forest Modells mit caret ziemlich lange dauert, haben wir uns entschieden, den Benchmark für mit caret trainierte Modelle nur 3 statt 10 mal durchzuführen.

Parameter für Gradient Boosting:
```{r modelInfo-gbm}
getModelInfo("gbm")$gbm$parameters
```


Benchmark für Gradient Boosting:
```{r caret-benchmark-gb}
benchmark(function(data) {
  caret::train(formula,
    data = data,
    method = "gbm",
    verbose = 0
  )
}, 3)
```

## Übersicht
package | Algorithmus
--------|------------
rForestry | Random Forest, Gradient Boosting
gbm | Gradient Boosting
caret | Random Forest, Gradient Boosting, u.v.m
