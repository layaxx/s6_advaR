---
title: 'Random Forest vs. Gradient Boosting'
author: "Jonathan Loew, Yannick Lang"
date: 
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    math: katex
    number_sections: yes
    toc: yes
    toc_depth: 2
    fig_caption: yes
    fig_width: 7
    fig_height: 7
    keep_md: yes
subtitle: 'verschiedene Random Forest / Gradient Boosting packages in R'
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Dataset
```{r}
library(pacman)

set.seed(42)

data <- MASS::Boston # data set to be used
column <- 14 # column of the data set that will be predicted
formula <- formula(paste(names(data)[column], "~ ."))
```

# Benchmark
Die Benchmark function erwartet els Eingabe eine Funktion, die das Model generiert und eine optionale Zahl, die der Anzahl der Wiederholungen enspricht.

Das Model wird `times` Male mit je unterschiedlichen Daten geschätzt und anhand der Testdaten auf Güte überprüft. Zurückgegeben werden durchschnittliche Standardabweichung/Durchschnitt der Differenz zwischen echten Werten des Testdatensatzes und den geschätzten Werten des Models und die durchschnittliche Laufzeit der Modelgenerierung.
```{r}
# auxiliary function which splits a dataset into training and test data
split_into_training_and_test <- function(dataset) {
  # 20% of data will be test data
  test_ids <- sample(nrow(data), nrow(data) * 0.2)

  list(test = data[test_ids, ], training = data[-test_ids, ])
}

get_x_values <- function(dataset) {
  dataset[, -column]
}
get_y_values <- function(dataset) {
  dataset[, column]
}

run_single_benchmark <- function(createModel) {
  datasets <- split_into_training_and_test(data)

  # calculates the model and times it
  start_time <- Sys.time()
  model <- createModel(datasets$training)
  end_time <- Sys.time()

  # calculates predicted values from model
  predicted_values <- predict(model, get_x_values(datasets$test))
  actual_values <- get_y_values(datasets$test)

  # compares predicted with actual values and returns results
  list(
    sd = sd(actual_values - predicted_values),
    mean = mean(actual_values - predicted_values),
    duration = end_time - start_time
  )
}


benchmark <- function(createModel, times = 1) {
  sums <- list(sd = 0, mean = 0, duration = 0)
  for (idx in 1:times) {
    # runs the actual benchmark `times` times
    result <- run_single_benchmark(createModel)

    # adds up the results
    sums$sd <- sums$sd + result$sd
    sums$mean <- sums$mean + result$mean
    sums$duration <- sums$duration + result$duration
  }

  # calculates the average over every run and returns the result
  averages <- list(
    sd = sums$sd / times,
    mean = sums$mean / times,
    duration = sums$duration / times
  )

  averages
}
```


# packages
## Rforestry
Supports both Random Forests and Gradient Boosting

```{r}
p_load(Rforestry)
```

Random Forest: uses `forestry` function

```{r}
benchmark(function(data) {
  forestry(x = get_x_values(data), y = get_y_values(data))
}, 10)
```

Gradient Boosting: uses `multiLayerForestry` function
```{r}
benchmark(function(data) {
  multilayerForestry(x = get_x_values(data), y = get_y_values(data))
}, 10)
```

## gbm
Gradient Boosting

```{r}

p_load(gbm)
```

Benchmark:
```{r}
benchmark(function(data) {
  gbm(formula,
    data = data
  )
}, 10)
```
 
## caret
via https://www.r-bloggers.com/2018/11/machine-learning-basics-gradient-boosting-xgboost/

Predictions:
```{r}
benchmark(function(data) {
  caret::train(formula,
    data = data,
    method = "gbm",
    # preProcess = c("scale", "center"),
    verbose = 0
  )
}, 10)
```
