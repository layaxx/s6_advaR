---
title: 'Random Forest vs. Gradient Boosting'
author: "Jonathan Loew, Yannick Lang"
date: 
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    math: katex
    number_sections: yes
    toc: yes
    toc_depth: 2
    fig_caption: yes
    fig_width: 7
    fig_height: 7
    keep_md: yes
subtitle: 'verschiedene Random Forest / Gradient Boosting packages in R'
---

```{r setupPackages, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require("pacman")) {
  # install pacman if not already installed
  options(repos = list(CRAN = "http://cran.rstudio.com/"))
  install.packages("pacman")
}
library(pacman)
```

# Implementierung in R

## Dataset
```{r dataset}
set.seed(42)

data <- MASS::Boston # data set to be used
column <- 14 # column of the data set that will be predicted
formula <- formula(paste(names(data)[column], "~ ."))
```

## Benchmark
Die Benchmark function erwartet els Eingabe eine Funktion, die das Model generiert und eine optionale Zahl, die der Anzahl der Wiederholungen enspricht.

Das Model wird `times` Male mit je unterschiedlichen Daten geschätzt und anhand der Testdaten auf Güte überprüft. Zurückgegeben werden durchschnittliche Standardabweichung, Differenz zwischen echten Werten des Testdatensatzes und den geschätzten Werten des Models und die durchschnittliche Laufzeit der Modelgenerierung.
```{r benchmark}
# auxiliary function which splits a dataset into training and test data
split_into_training_and_test <- function(dataset) {
  # 20% of data will be test data
  test_ids <- sample(nrow(data), nrow(data) * 0.2)

  list(test = data[test_ids, ], training = data[-test_ids, ])
}

get_x_values <- function(dataset) {
  dataset[, -column]
}
get_y_values <- function(dataset) {
  dataset[, column]
}


run_single_benchmark <- function(createModel) {
  datasets <- split_into_training_and_test(data)

  # calculates the model and times it
  start_time <- Sys.time()
  model <- createModel(datasets$training)
  end_time <- Sys.time()

  # calculates predicted values from model
  predicted_values <- predict(model, get_x_values(datasets$test))
  actual_values <- get_y_values(datasets$test)

  # compares predicted with actual values and returns results
  list(
    sd = sd(actual_values - predicted_values),
    mean = abs(mean(actual_values - predicted_values)),
    duration = end_time - start_time
  )
}


benchmark <- function(createModel, times = 1) {
  sums <- list(sd = 0, mean = 0, duration = 0)
  for (idx in 1:times) {
    # runs the actual benchmark `times` times
    result <- run_single_benchmark(createModel)

    # adds up the results
    sums$sd <- sums$sd + result$sd
    sums$mean <- sums$mean + result$mean
    sums$duration <- sums$duration + result$duration
  }

  # calculates the average over every run and returns the result
  averages <- list(
    sd = sums$sd / times,
    mean = sums$mean / times,
    duration = sums$duration / times
  )

  averages
}
```


## packages
### Rforestry
Unterstützt sowohl Random Forests als auch Gradient Boosting

```{r rforestry-setup}
p_load(Rforestry)
```

Random Forest: mithilfe der `forestry` Funktion:

Tuning Parameter [@R-Rforestry]:

- ntree: Anzahl der Bäume, default 500
- maxDepth: maximale Tiefe eines Baums, default 99

```{r rforestry-benchmark}
benchmark(function(data) {
  forestry(x = get_x_values(data), y = get_y_values(data))
}, 10)
```

Gradient Boosting: mithilfe der `multiLayerForestry` Funktion:

Tuning Parameter [@R-Rforestry]:

- ntree: Anzahl der Bäume, default 500
- maxDepth: maximale Tiefe eines Baums, default 99
- eta: Lernrate, default 0.3
```{r rforestry-benchmark-2}
benchmark(function(data) {
  multilayerForestry(x = get_x_values(data), y = get_y_values(data))
}, 10)
```

### gbm
Gradient Boosting

Tuning Parameter [@R-gbm]:

- n.trees: Anzahl der Bäume, default 100
- interaction.depth: maximale Tiefe eines Baums, default 1
- shrinkage: Lernrate, default 0.1
- n.minobsinnode: Mindestanzahl an Beobachtungen in Knoten, default 10

```{r gbm-setup}
p_load(gbm)
```

Benchmark:
```{r gbm-benchmark, message=FALSE}
benchmark(function(data) {
  gbm(formula,
    data = data,
    distribution = "gaussian"
  )
}, 10)
```
 
### caret
Sehr vielseitige Bibliothek, die verschiedenste Modelle schätzen kann.

Standard Parameter:

- NAs: schlägt fehl, wenn die Daten NAs enthalten, kann geändert werden

Tuning Paremeter hängen von der Funktion die zum Schätzen verwendet werden soll ab.

Diese können allerdings automatisch getuned werden über die Parameter trControl und tuneLength [@R-caret].
Für genauere Kontrolle über die erlaubten Werte kann auch tuneGrid benutzt werden, hierfür werden allerdings die Namen der Parameter gebraucht, diese können mit folgendem Code ausgegeben werden:
```{r caret-setup}
p_install("ggplot2", force = FALSE)
p_load(caret)
getModelInfo("rf")$rf$parameters
getModelInfo("gbm")$gbm$parameters
```

Benchmark:
```{r caret-benchmark}
benchmark(function(data) {
  caret::train(formula,
    data = data,
    method = "gbm",
    verbose = 0
  )
}, 10)
```

## Übersicht
package | Algorithmus
--------|------------
rForestry | Random Forest + Gradient Boosting
gbm | Gradient Boosting
caret | Random Forest, Gradient Boosting u.v.m
